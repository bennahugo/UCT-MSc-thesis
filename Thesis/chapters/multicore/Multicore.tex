\chapter{Review of multi- and many-core processing models}
\section{Multicore CPU architectures}
\subsection{Switch to MIMD processing paradigm}
Traditionally software development was geared towards Single Instruction Single Data (SISD) processing architectures, where instructions were executed in the same 
order they were issued to the processor. For a long period this approach worked well; the growth in the number of transistors in processors and associated 
advancements in cache, hardware scheduling and execution logic meant that software kept becoming faster without the need for any serious modifications. Processor clock
speeds continued to increase dramatically throughout the 1980s to the early 2000s (see Figure~\ref{fig_clocks}), driving enormous growth in the capabilities of both
desktop and server computers. However, since processor power usage is a function of the clock rate, the significant increase in the achievable processor clock rates 
also brought about increased power consumption and associated heat dispersion problems. Hardware manufacturers could no longer continue to drop the input 
voltages to processors to successfully overcome this power requirement and CPU clockrates stagnated. This brought about a sea change in the software industry: 
future software can no longer rely on significant improvements stemming solely from hardware improvements. Since 2002 the response time of programs have slowed 
from a factor 50\% decrease to less than a 20\% decrease per year (see Figure~\ref{fig_runtime_decrease_cpu}).

\begin{figure}[ht!]
 \begin{mdframed}
  \centering
  \includegraphics[width=0.8\textwidth]{images/cpu_clocks.png}
  \caption[Power wall]{The growth in CPU clock rates for 8 generations of x86 Intel processors and associated power consumption. By reducing input voltages to the processors
  engineers were able to keep power requirements low while increasing clock frequencies throughout the 1980s and 1990s. Limitations in semiconductor technology has, however,
  limited further increases in clock cycle rate.}
  \label{fig_clocks}
 \end{mdframed}
\end{figure}

\begin{figure}[ht!]
 \begin{mdframed}
  \centering
  \includegraphics[width=0.8\textwidth]{images/runtime_decrease_cpu.png}
  \caption[Program response time decline]{Here processor performance is plotted relative to the VAX-11/780 measured by the SPECint benchmarks. The substantial growth seen after the mid-1980s
  are primarily due to increasingly advanced processor architectures. Since 2002 the growth has been slowed to below 25\% primarily due to the power wall and high memory latencies.}
  \label{fig_runtime_decrease_cpu}
 \end{mdframed}
\end{figure}

Since the latter half of the 2000s most desktop processors started subscribing to the Multiple Instruction Multiple Data (MIMD) paradigm where several processes 
(or threads of execution within a single process) is executed simultaneously within multiple \textit{microprocessors} on the same processor die (also known as processing cores). 
See Figure~\ref{fig_amd_barcelona_arch} for an example of such a multi-core layout. Modern processors is thus geared towards attaining higher throughput by better exploiting the 
abundance of task-level parallelism in modern day computer usage, instead of increasing the response rate of individual processes. Whereas once only a select number of compute 
programs warranted nodes with multiple processors (physical chips) per compute node, today any desktop software has to be able to use multiple compute streams (``threads'') 
in order to fully exploit the compute capacity of modern hardware. 
\begin{figure}[ht!]
 \begin{mdframed}
  \centering
  \includegraphics[width=0.8\textwidth]{images/amd_barcelona.png}
  \caption[AMD Barcelona]{Microphotograph and layout of the quad-core AMD Barcelona processor. It is common that each of the cores have their own private caches as well as a shared
  cache between cores.}
  \label{fig_runtime_decrease_cpu}
 \end{mdframed}
\end{figure}

\section{Many core GPU architectures}
Much of the discussion in this chapter is drawn from Kirk and Hwu \cite[ch. 1-3]{kirk2012programming}, Owens et al. \cite{owens2008gpu} and the CUDA
programming reference \cite{cuda}.
\subsection{Historical development}
Today's modern programmable GPU devices have evolved from the fixed-pipeline graphics hardware of the 1980's and 1990's. Driven by the demand
for high resolution graphics in the video game industry modern devices must be able to render billions of pixels per second (72 giga pixels per second 
for the latest Maxwell GTX980 devices \cite{gtx980}). As such these devices are geared towards achieving high throughput over higher operational latencies compared to traditional
CPU-based computing. The steps involved with transforming primitives (typically triangles) in world space to rasterized images rendered by a display takes thousands
of compute cycles from start to finish. However the coordinate, lighting and per-pixel shading operations are highly data-parallel operations. In addition the stages within
the graphics pipeline can be computed in parallel; while new primitives enter the pipeline the rasterization and fragment processing of primatives previously transformed is 
completed, enabling both data- and task-parallelism on GPUs. The major steps in the graphics pipeline is shown in Figure~\ref{fig_graphics_pipeline} for reference.

\begin{figure}[ht!]
 \begin{mdframed}
 \centering
  \begin{tikzpicture}[node distance=2.8cm,  
    block/.style={
      draw,
      fill=white,
      rectangle, 
      text width=2.0cm,
      inner sep=0.2cm}]
    \node (app) [block] {3D application / game};
    \node (api) [block, right of=app] {3D API \& drivers};
    \node (fe) [block, below of=app] {GPU front end};
    \node (assembly) [block, right of=fe] {Primitive assembly};
    \node (rasterization) [block, right of=assembly] {Rasterization and interpolation};
    \node (rasterops) [block, right of=rasterization] {Raster operations};
    \node (fb) [block, right of=rasterops] {Framebuffer};
    \draw [dashed,transform canvas={yshift=-1.4cm}] ($(app) - (0.5cm,0)$) -- (fb |- app) node [right, above] (cpu) {CPU} node [right, below] (gpu) {GPU};
    \draw [rarrow] (app) -- (api);
    \draw [rarrow] (api) -- (fe);
    \draw [rarrow] (fe) -- (assembly);
    \draw [rarrow] (assembly) -- (rasterization);
    \draw [rarrow] (rasterization) -- (rasterops);
    \draw [rarrow] (rasterops) -- (fb);
  \end{tikzpicture}
 \caption[Graphics pipeline]{The classical graphics pipeline. At the front end of the pipeline verticies are transformed, colours assigned,
 texture coordinates and normals calculated. The rasterization step interpolate per-vertex data, such as colour accross all the pixels 
 touched by the triangle formed between vertex tripples. Raster operations performs final blending and anti-aliasing operations, before the images written
 out to the display frame buffer.}
 \label{fig_graphics_pipeline}
 \end{mdframed}
\end{figure}

The requirement of transforming, rasterizing and shading possibly millions of triangle primitives driving GPU development clearly diverges from the requirements behind the
design of traditional CPUs. CPU development is driven by the need to process large sequential programs per CPU core, each containing complex branching and diverse memory access patterns, 
whereas GPUs are driven by the need to apply the same set of basic operations to many elements (or ``Single Instruction Multiple Data'' [SIMD]) paradigm that graphics processing subscribes to. 
Whereas CPUs use several tiers of large caches to hide the latencies of memory accesses GPUs have relatively little on-chip cache memory per basic compute (``Streaming Processor'' [SP]) 
unit. Instead of caching GPUs rely mostly on the amount of parallel work available to each SP unit and a fast context switching / work scheduling mechanism to ensure each of the SPs are 
occupied with work while memory transactions are completed for threads stalled by load and store operations.

Although the hardware platform could potentially be employed for applications other than 3D graphics it wasn't until the early 2000's that programmable graphics hardware became widely
available. For instance the NVIDIA GeForce 3 exposed the internal instruction set of the vertex processor to the application developer. Soon the ATI Radion 9700 and GeForce FX 
made the fragment shading process normally part of the rasterization and interpolation step reprogrammable. At this point the vertex and fragment shading units were run on seperate
hardware. The XBox 360 (2005) introduced an early unified vertex and fragment shader processor.

Even though the hardware now supported extending the traditional graphics pipeline to do more complex operations it was still impractical to use the highly parallel hardware for 
computational processes other than graphics on any large scale products. Computational problems had to be mapped to the standard graphics operands: verticies and textures. Other problems
such as the lack of scattered memory writes limited the applicability of the platform to a small number of problems.

This was addressed by a series of software (along with necessary hardware modifications) developments that aimed to to give application developers access to the processors without having to call
on Graphics APIs. Of those developments BrookGPU \cite{buck2004brook} was an early abstraction away from graphics primatives. It recasted computation in terms of small programs (``kernels'') operating
on ``streams'' of input data elements (arrays of values that can be operated on in parallel). This paradigm sets GPUs aside from ordinary vector processors that loads a series of values from global
memory, performs a simple mathematical operation on each of the elements and write the results back to global memory. Instead stream processors can load values from local register memory, performing multiple
operations on each of these values before storing the results (possibly to local memory). The paradigm allows for achieving greater arithmetic intensity (a single memory operation is followed
by many computations) and is critical for hiding the high latencies of memory accesses experienced on GPUs. 

More recently NVIDIA's CUDA \cite{cuda} and the OpenCL \cite{opencl} have become widely used in programming GPUs to perform general scientific computing used in a variety of fields including the signal 
processing domain. Both lend themselves to the streaming processor paradigm and facilitate the implementation
of the following generally-used parallel computing primitives:
\begin{itemize}
 \item Scatter/gather: The addresses used in memory accesses (both load and store) can be computed.
 \item Map: an operation is applied to every element in the stream. Typically many threads will be launched
       each reading an element from the stream, performing the operation on that element and writing the value
       back to memory afterwards.
 \item Reduce: By applying a binary associative operation repeatedly a sequence of values is reduced to a single value.
       An example is ordinary summation, minimums/maximums, variance, etc. These operations typically split the data up into
       subsets, performing many additions in parallel and repeating the process on the set of results until a single value is
       obtained. 
 \item Scans: Scan and prefix scan operations are widely used in parallel programming (for instance index calculations as used
       in our work). A scan of an array contains the running accumulations (in the case of summation) of elements in an array. 
\end{itemize}

\subsection{Modern programmable GPU architecture}
The difference in design philosophy is reflected in the substantial differences in the architectures of modern GPUs and CPUs. CPUs dedicate a significant portion die area to large caches and complex control logic dealing with
branch prediction and scheduling. GPUs on the other hand dedicate a significant portion of die area to arithmetic and other execution units and rely on having enough arithmetic work to occupy most of those units most of the time.
Figure~\ref{fig_cpu_gpu_diff} illustrates the proportions of both CPU and GPU die area spent on arithmetic.
\begin{figure}[ht!]
 \begin{mdframed}
  \centering
  \includegraphics[width=0.5\textwidth]{images/gpu-devotes-more-transistors-to-data-processing.png}
  \caption[CPU vs. GPU architecture]{The proportion of transistors allocated to arithmetic processing in CPUs and GPUs. Taken from the NVIDIA programming reference \cite{cuda}}
  \label{fig_cpu_gpu_diff}
 \end{mdframed}
\end{figure}

Modern GPUs works best in problem contexts where a significant portion of the computation is data-parallel. In CUDA nomenclature (similar concepts exist in OpenCL) the parallel work is broken up into a \textit{grid} of
separate thread \textit{blocks} (see Figure~\ref{fig_grid_blocks} for an illustration). In each of these blocks on-chip memory resources is shared, limiting the size of the individual blocks. On current NVIDIA GPUs there is a hard limit (1024) to the number of threads size, 
however the amount of special memory (including register memory) available to each of these blocks may further limit the number threads in the block that can physically execute simulaneously. Each of the 
blocks would therefore process a portion of stream memory. Blocks are in turn subdivided into \textit{warps} of threads (currently 32 threads form a warp) that execute instructions in lockstep. This means that when one
thread inside a warp is stalled (for instance a memory access or a synchronization barrier) the entire warp is stalled. Unlike the instruction scheduling units of CPUs the schedulers in GPUs also don't contain complex branch prediction logic; when some
threads in a warp require the execution of one of the directions in a branch while the rest take another direction both sides of the branch is evaluated and the results are simply masked out for the threads that are unaffected
by the branch. This places a hefty penalty on branch divergence within the instruction kernels and memory accesses that don't adhere to the alignment specifications of the GPU. Note that this description of how work is layered out in a high level programming language is independent of specifics of the device the work is to be run on. 
The individual blocks can be mapped onto the targeted device in any order and in any quantity, depending on the resources of the device. Each block of work should therefore perform its computation in isolation of the remaining blocks. Although intra-block communication between threads 
and synchronization is possible, inter-block communication is only possible through accesses to off-chip memory and should be avoided.
\begin{figure}[ht!]
 \begin{mdframed}
  \centering
  \includegraphics[width=0.45\textwidth]{images/grid-of-thread-blocks.png}
  \caption[Thread layout in CUDA]{The layout of work threads in CUDA. Taken from the NVIDIA programming reference \cite{cuda}}
  \label{fig_grid_blocks}
 \end{mdframed}
\end{figure}

At a hardware level GPUs comprise of multiple \textit{Streaming Multiprocessors}, each containing many \textit{Stream Processors} capable of performing arithmetic (predominantly IEEE 754 single precisision floating point) along
with several special function units, warp-schedulers and memory load/store units. Figure~\ref{fig_kepler_arch} shows the layout of Kepler-generation NVIDIA GPUs. The exact number of Streaming Multiprocessors and Stream Processors
vary between generations of GPUs, but the total number of Streaming Multiprocessors per GPU tend to double every 2 years. Depending on resource constraints of each of the thread blocks several blocks may be mapped to a single 
Streaming Multiprocessor for simultaneous execution. The warp schedulers schedule warps that have instructions (along with the required operand data) ready for execution onto sets of Stream Processors, dispatching independent 
instructions onto individual Stream Processors per clock cycle (depending on the number of dispatch units available per Multiprocessor). Warps that are stalled (for example waiting on a load/store operation) is switched out of context
and replaced with warps that have operands ready for processing, thereby hiding memory access latencies. Ideal kernels should therefore:
\begin{itemize}
 \item Contain enough independent arithmetic instructions to occupy all the Stream Processors during
       any given clock cycle.
 \item Access to both on- and especially off-chip memory should be kept to a minimum and subscribe to coalesced access patterns, especially considering that the number of load/store units are far fewer than the number of
       single precision units and peak memory bandwidth is on the order of 60x lower (Kepler generation) than peak single precision compute throughput of the device. In other words the arithmetic intensity should be high, as
       is the case in typical graphics shading operations for instance.
 \item There must be enough warps of work scheduled to the GPU to keep the Multiprocessors occupied most of the time.
 \item Special memory resources (including registers) should be used sparingly to ensure the Streaming Multiprocessor is not starved of resources as this lowers the number of warps that can be executed at any given point in time 
      (``effective occupancy'').
\end{itemize}
\begin{figure}[ht!]
 \begin{mdframed}
  \centering
  \begin{subfigure}[b]{0.7\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/kepler_arch.png}
    \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.6\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/kepler_smx_arch.png}
    \caption{}
  \end{subfigure}
  \caption[Kepler architecture]{Kepler die architecture. (a) shows the overall die layout, containing 15 Streaming Multiprocessors. (b) shows the layout of each multiprocessor, containing one double precision unit for every 
				3 single precision units, 1 load/store and 1 special function unit for every 6 single precision units. The total number of registers, local and shared memory is split between the number of threads per 
				block, determining how many blocks can be executed simultaneously. Taken from the Kepler whitepaper \cite{kepler}}
  \label{fig_kepler_arch}
 \end{mdframed}
\end{figure}

\subsection{GPU memory layout}
The GPU memory hirarchy is substantially different from those found on CPUs. As we pointed out earlier the total cache memory
per Multiprocessor is divided between many Stream Processors. The following special memory systems exist on the CPU:
\begin{itemize}
 \item Shared instruction cache. GPUs are SIMD devices by nature and thus requires the same set of instructions to be executed
 by many Stream Processors. This means that the instruction cache can be shared between many Stream Processors, unlike with CPUs
 where each core has its own instruction cache.
 \item Local memory: Each Stream Processor has access to its own private local memory space in which register memory reside. The total number of
 registers is divided amongst all threads and is determined by the maximum number of registers needed to execute the instructions contained in a 
 kernel, setting the upper limit to how many threads can be executed on the Multiprocessor at any point in time. 
 \item L1 data cache: The L1 cache on GPUs are split into a local cache and a shared memory cache. On GPUs the local cache stores register 
 spills from local memory. Depending on the hardware generation L1 memory also caches accesses to global memory (2.x by default), but this is
 not necessarily done by all devices. Some 3.x Keper devices can opt in to this behavior by specifying compile time options.
 \item Shared memory: As suggested shared memory is a cache memory shared between all threads executing in a block. The split of the L1 cache into
 shared and data caches is reconfigurable at run-time. It is also important to note that the exact amount of shared memory requested by each kernel at 
 launch sets the limit on how many blocks of that kernel can be executed simultaneously. Shared memory is used for both communication between threads as well
 as storing values that are commonly accessed (and/or modified) by multiple threads. Importantly for performance, however, it is worth mentioning that
 shared memory is divided in banks; each consecutive word falls into a different bank. An out of sequential order or strided reads between two or more threads
 can therefore result in trying to access the same bank simultaneously, generating a bank conflict. Special rules apply for sub-word and multiple-word accesses, for
 which the reader can refer to the CUDA API \cite[Section G: Compute Capabilities]{cuda}.
 \item Constant cache: Constant memory is read-only memory that resides in off-chip memory, but is the accesses are cached. CUDA uses this mechanism to broadcast
 a single read to a half-warp of threads thereby saving 15/16 of the memory accesses that would be encountered when the same read pattern is made to global memory.
 Consecutive accesses to the same memory encounters no extra cost.
 \item Read-only data (texture) cache: In graphics processing one of the most common operations performed by the GPU is to map textures onto triangle primitives. 
 Memory reads to textures (stored off-chip) is highly regular and spatially coherent, meaning that a group of work units will likely read values from the same area of texture
 memory. The caching mechanism is designed to optimize this access pattern. The texture cache resides within each Multiprocessor and is not kept up to date with changes made to
 textures and can therefore become stale. Loads from texture memory can also be made with hardware-based interpolation enabled.
 \item Global memory: Global memory resides off chip, just like constant and texture memory. Global memory accesses
 are cached in the crossbar L2 cache that is shared between Multiprocessors and has a cache-line length of 32-bytes. In Fermi accesses were further cached in the L1
 data cache by default in which case the cache lines were 128 bytes in length. With some compute 3.x hardware this caching can be enabled. The compiler also cache 
 some read-only accesses in the read-only data cache in compute 3.x devices. Warp memory accesses (not essentially ordered on an intra-warp level) that are aligned 
 with these boundaries maximizes available memory bandwidth.
\end{itemize}
